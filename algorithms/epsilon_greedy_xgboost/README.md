# Dynamic Pricing with XGBoost Demand Forecasting & Multi-Armed Bandits

This project analyzes a year's worth of electronics sales data to uncover purchasing patterns, build a predictive model for product demand, and simulate a dynamic pricing strategy using a Multi-Armed Bandit (MAB) agent.

The goal is to move from static, fixed pricing to an intelligent system that can learn and adapt to find the optimal price point that maximizes revenue.

## Table of Contents

- [Project Overview](#project-overview)
- [Project Structure](#project-structure)
- [Key Components](#key-components)
  - [1. Exploratory Data Analysis (EDA)](#1-exploratory-data-analysis-eda)
  - [2. Demand Forecasting Model](#2-demand-forecasting-model)
  - [3. Dynamic Pricing Simulation (MAB)](#3-dynamic-pricing-simulation-mab)
- [Reusable Code (src)](#reusable-code-src)
- [How to Run This Project](#how-to-run-this-project)
- [Key Findings & Results](#key-findings--results)
- [Future Work: The Contextual Bandit](#future-work-the-contextual-bandit)

## Project Overview

This project is broken into three main phases:

1. **Analysis**: We first dig into the historical sales data (`data/raw/`) to answer key business questions: What's our best month for sales? Which city buys the most? What time of day should we advertise?

2. **Forecasting**: We use the insights from our analysis to build a machine learning model (XGBoost) that can predict the `Quantity Ordered` of a product based on features like the product type, city, month, and hour.

3. **Simulation**: We create a simple, simulated market environment to test a "smart" pricing agent. This agent (an Epsilon-Greedy MAB) knows nothing about the market and must learn the best price for a product by "exploring" different price points and "exploiting" the ones that produce the most revenue.

The ultimate goal is to combine Forecasting (2) and Simulation (3) to create a **Contextual Bandit**, which is a pricing agent that can use the XGBoost model to predict the best price given the current context (e.g., "What's the best price for an iPhone in San Francisco in December?").

## Project Structure

```
algorithms/epsilon_greedy_xgboost/
│
├── data/
│   ├── raw/                  # <-- Original, untouched .csv files
│   ├── interim/              # <-- Merged raw data (intermediate step)
│   └── processed/
│       └── sales_cleaned.csv # <-- Final, cleaned data for modeling
│
├── models/                   # <-- Saved model files
│   └── xgb_demand_forecaster.json # <-- Generated by Notebook 02
│
├── notebooks/
│   ├── 01_EDA_and_Visualization.ipynb
│   ├── 02_Demand_Forecasting.ipynb
│   └── 03_MAB_Simulation.ipynb
│
├── reports/
│   └── figures/              # <-- All .png graphs are saved here
│
├── src/
│   ├── environment.py        # <-- Reusable PricingEnvironment class
│   └── epsilon_greedy.py     # <-- Reusable EpsilonGreedyAgent class
│
├── .gitignore
├── README.md
└── requirements.txt
```

## Key Components

### 1. Exploratory Data Analysis (EDA)

**File**: `notebooks/01_EDA_and_Visualization.ipynb`

This notebook handles all data ingestion, cleaning, and initial analysis.

**Purpose**: To merge all 12 monthly sales CSVs from `data/raw/` into a single, clean dataset.

**Key Steps**:

- Loads and merges all raw `.csv` files
- Handles NaN values by dropping empty rows
- Converts columns to correct data types (`Quantity Ordered` to numeric, `Order Date` to datetime)
- Performs **Feature Engineering** to extract `Revenue`, `Month`, `Hour`, `DayOfWeek`, and `City` (from `Purchase Address`)

**Key Analyses Performed**:

- **Monthly Revenue**: Visualizes sales performance across the year (finds December is the peak)
- **City-wise Revenue**: Identifies the top-performing cities (San Francisco is #1)
- **Hourly Demand**: Shows the peak hours for orders (12:00 PM and 7:00 PM)
- **Top Products**: Compares products by both quantity sold and total revenue
- **Price vs. Demand**: A log-log plot showing the general inverse relationship between a product's price and its sales volume

**Output**:

- `data/processed/sales_cleaned.csv`
- All plots saved to `reports/figures/` (e.g., `01_monthly_revenue.png`, `02_city_revenue.png`, etc.)

### 2. Demand Forecasting Model

**File**: `notebooks/02_Demand_Forecasting.ipynb`

This notebook builds a predictive model to forecast demand (`Quantity Ordered`).

**Purpose**: To prove we can accurately predict how many units of a product will be sold given a specific context.

**Input**: `data/processed/sales_cleaned.csv`

**Model**: An XGBoost Regressor is used for its high performance

**Features (Context)**: `Product`, `City`, `Month`, `Hour`, `DayOfWeek`

**Target (Demand)**: `Quantity Ordered`

**Evaluation**: The model is trained on 80% of the data and tested on 20%. It achieves a very low RMSE of ~0.461 units, meaning its predictions are, on average, extremely close to the true quantity ordered.

**Output**: The trained model is saved to `models/xgb_demand_forecaster.json`

### 3. Dynamic Pricing Simulation (MAB)

**File**: `notebooks/03_MAB_Simulation.ipynb`

This notebook simulates a simple dynamic pricing problem to test our `EpsilonGreedyAgent`.

**Purpose**: To demonstrate that a MAB agent can learn an optimal price in a controlled environment without any prior knowledge.

**Note**: This simulation does not use the XGBoost model. It uses a simple, pre-defined "true" demand curve.

**Modules**: Imports the `PricingEnvironment` and `EpsilonGreedyAgent` from the `src/` directory.

**Environment**: A `PricingEnvironment` is created with 10 possible prices ($10 to $100). We define a simple "hidden" demand curve (`200 - 3 * price`) where the true optimal price is `$30.00`, yielding a max revenue of `$3300.00`.

**Agent**: The `EpsilonGreedyAgent` is set with `epsilon=0.1`. This means it has a 10% chance to "explore" (pick a random price) and a 90% chance to "exploit" (pick the price it currently thinks is the best).

**Key Analyses Performed** (for 50, 100, 200, and 1000 rounds):

- **Learning Curve**: Shows the agent's average revenue per round, which quickly converges towards the true maximum of $3300
- **Learned vs. True Revenue**: A line plot comparing the agent's final estimated revenue for each price vs. the hidden truth. This shows the agent's estimates become very accurate
- **Price Selection Frequency**: A bar chart showing how many times each price was chosen. It clearly shows the agent learns to favor the optimal $30.00 price
- **Cumulative Regret**: A critical MAB metric showing the total "lost revenue" from exploration (i.e., `true_max_revenue - reward_from_chosen_price`). This plot shows that regret flattens over time, indicating the agent has successfully learned and is no longer making costly "mistakes"

## Reusable Code (src)

- **`src/environment.py`**: Contains the `PricingEnvironment` class. This class represents the "market" and is responsible for calculating the demand and reward (revenue) for a given price.

- **`src/epsilon_greedy.py`**: Contains the `EpsilonGreedyAgent` class. This class holds the logic for the pricing agent.
  - `select_arm()`: Implements the epsilon-greedy logic (explore vs. exploit)
  - `update()`: Updates the agent's internal "value" (estimated revenue) for a chosen price based on the reward it received

## How to Run This Project

Follow these steps to replicate the analysis and results.

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/your-username/your-repo.git
   cd your-repo/algorithms/epsilon_greedy_xgboost
   ```

2. **Set up Virtual Environment (Recommended)**:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Notebooks in Order**:
   You must run the notebooks sequentially, as they depend on each other.

   - **First**: Open and run `notebooks/01_EDA_and_Visualization.ipynb`

     - **Input**: `data/raw/` .csv files
     - **Output**: `data/processed/sales_cleaned.csv` and all plots in `reports/figures/`

   - **Second**: Open and run `notebooks/02_Demand_Forecasting.ipynb`

     - **Input**: `data/processed/sales_cleaned.csv`
     - **Output**: `models/xgb_demand_forecaster.json`

   - **Third**: Open and run `notebooks/03_MAB_Simulation.ipynb`
     - **Input**: `src/environment.py`, `src/epsilon_greedy.py`
     - **Output**: All MAB-related plots in `reports/figures/`

## Key Findings & Results

**EDA**:

- **Peak Month**: December is the highest-revenue month, likely due to holiday shopping
- **Top City**: San Francisco is the #1 city for sales
- **Peak Hours**: Demand peaks around 12:00 PM (lunchtime) and 7:00 PM (19:00)

**Forecasting**:

- The XGBoost model can predict the `Quantity Ordered` with high accuracy (RMSE: 0.461)
- The most important features for predicting demand are the specific `Product` and `City`

**Simulation**:

- The Epsilon-Greedy agent successfully learns and converges to the true optimal price of $30.00 after ~1000 rounds
- This demonstrates that a MAB approach is a viable strategy for optimizing price, as the agent can find the best price with no prior knowledge, minimizing "lost revenue" (regret) over time
